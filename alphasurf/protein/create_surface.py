import os
import platform
import shutil
import subprocess
import sys
import tempfile
from pathlib import Path

import numpy as np
import open3d as o3d
import pandas as pd
import trimesh

# Add cgal_alpha_bindings to Python path
# Use CGAL_BINDINGS_DIR env var if set, otherwise use default build/ location
_cgal_bindings_dir = os.environ.get("CGAL_BINDINGS_DIR")
if _cgal_bindings_dir is None:
    _current_dir = Path(__file__).resolve().parent
    _cgal_bindings_path = _current_dir.parent.parent / "cgal_alpha_bindings" / "build"
    _cgal_bindings_dir = str(_cgal_bindings_path)

sys.path.insert(0, _cgal_bindings_dir)
print(f"[CGAL] Loading bindings from: {_cgal_bindings_dir}")
import cgal_alpha  # ty:ignore[unresolved-import]

print(f"[CGAL] Successfully loaded: {cgal_alpha.__file__}")
from alphasurf.protein.graphs import parse_pdb_path

# os.environ['OMP_NUM_THREADS'] = '1'
# os.environ['MKL_NUM_THREADS'] = '1'
# os.environ['NUMEXPR_NUM_THREADS'] = '1'
if __name__ == "__main__":
    script_dir = os.path.dirname(os.path.realpath(__file__))
    sys.path.append(os.path.join(script_dir, "..", ".."))

from alphasurf.utils.python_utils import silentremove


def cluster_triangles_by_vertex_sharing(faces):
    """
    Cluster triangles that share ANY vertex (not just edges).

    More robust for alpha_complex surfaces with linkers/flexible regions.

    Args:
        faces: (N, 3) array of triangle vertex indices

    Returns:
        triangle_clusters: (N,) array of cluster IDs
        cluster_n_triangles: (K,) array of triangle counts per cluster
    """
    from collections import defaultdict

    n_faces = len(faces)

    # Build vertex-to-faces map
    vertex_to_faces = defaultdict(list)
    for face_idx, face in enumerate(faces):
        for vertex_idx in face:
            vertex_to_faces[vertex_idx].append(face_idx)

    # Union-Find data structure
    parent = list(range(n_faces))

    def find(x):
        if parent[x] != x:
            parent[x] = find(parent[x])  # Path compression
        return parent[x]

    def union(x, y):
        root_x = find(x)
        root_y = find(y)
        if root_x != root_y:
            parent[root_x] = root_y

    # Connect triangles that share any vertex
    for face_list in vertex_to_faces.values():
        if len(face_list) > 1:
            # All faces in this list share a vertex
            for i in range(1, len(face_list)):
                union(face_list[0], face_list[i])

    # Assign cluster IDs
    cluster_map = {}
    triangle_clusters = np.zeros(n_faces, dtype=np.int32)

    for face_idx in range(n_faces):
        root = find(face_idx)
        if root not in cluster_map:
            cluster_map[root] = len(cluster_map)
        triangle_clusters[face_idx] = cluster_map[root]

    # Count triangles per cluster
    n_clusters = len(cluster_map)
    cluster_n_triangles = np.zeros(n_clusters, dtype=np.int32)
    for cluster_id in triangle_clusters:
        cluster_n_triangles[cluster_id] += 1

    return triangle_clusters, cluster_n_triangles


def save_debug_ply(mesh, base_name, step_name):
    """
    Save a debug .ply file at a specific processing step for visualization and debugging.
    """
    debug_filename = f"{base_name}_{step_name}.ply"
    try:
        o3d.io.write_triangle_mesh(debug_filename, mesh)
        print(
            f"Debug: Saved {debug_filename} ({len(mesh.vertices)} verts, {len(mesh.triangles)} faces)"
        )
    except Exception as e:
        print(f"Warning: Failed to save debug file {debug_filename}: {e}")


"""
In this file, we define functions to make the following transformations :
PDB -> MSMS surfaces as vert and faces 
MSMS surface -> refined one, saved as .ply
"""


def parse_verts(vert_file, face_file, keep_normals=False):
    """
    Generate the vertices and faces (and optionally the normals) from .vert and .face files generated by MSMS
    :param vert_file:
    :param face_file:
    :param keep_normals:
    :return:
    """
    with open(vert_file, "r", errors="ignore") as f:
        # Parse the file and ensure it looks sound
        lines = f.readlines()
        n_vert = int(lines[2].split()[0])
        no_header = lines[3:]
        assert len(no_header) == n_vert

        # Parse the info to retrieve vertices and normals
        lines = [line.split()[:6] for line in no_header]
        lines = np.array(lines).astype(np.float32)
        verts = lines[:, :3]
        if keep_normals:
            normals = lines[:, 3:6]

    with open(face_file, "r", errors="ignore") as f:
        # Parse the file and ensure it looks sound
        lines = f.readlines()
        n_faces = int(lines[2].split()[0])
        no_header = lines[3:]
        assert len(no_header) == n_faces

        # Parse the lines and remove 1 to get zero based indexing
        lines = [line.split() for line in no_header]
        lines = np.array(lines).astype(np.int32)
        faces = lines[:, :3]
        faces -= 1

    if keep_normals:
        return verts, faces, normals
    else:
        return verts, faces


def pdb_to_surf(pdb_path, out_name=None, density=1.0, remove_files=True):
    """
    Runs msms on the input PDB file and dumps the output in out_name
    :param pdb_path:
    :param out_name:
    :return:
    """
    if out_name is None:
        # Avoid writing into the dataset's pdb directory; use a temp location instead
        out_name = os.path.join(tempfile.gettempdir(), Path(pdb_path).stem)
    out_name = str(Path(out_name).with_suffix(""))
    vert_file = out_name + ".vert"
    face_file = out_name + ".face"
    xyzr_name = f"{out_name}_temp.xyzr"
    log_name = f"{out_name}_msms.log"

    script_dir = os.path.dirname(os.path.realpath(__file__))
    binary_base_path = os.path.abspath(os.path.join(script_dir, "..", "..", "bin"))

    # Auto-detect platform for MSMS binaries
    system = platform.system()
    if system == "Darwin":  # macOS
        platform_dir = "msms_macos"
    elif system == "Linux":
        platform_dir = "msms_linux"
    elif system == "Windows":
        platform_dir = "msms_windows"
    else:
        raise RuntimeError(f"Unsupported platform: {system}")

    binary_path = os.path.join(binary_base_path, platform_dir)
    msms_path = os.path.abspath(os.path.join(binary_path, "msms"))
    pdb2xyzr_path = os.path.abspath(os.path.join(binary_path, "pdb_to_xyzr"))
    success = False
    try:
        # First get the xyzr file (run from bin so pdb_to_xyzr finds atmtypenumbers reliably)
        with open(xyzr_name, "w") as f:
            subprocess.run(
                [pdb2xyzr_path, pdb_path], stdout=f, cwd=binary_path, check=True
            )

        # Then run msms on this file
        msms_args = [
            msms_path,
            "-if",
            xyzr_name,
            "-of",
            out_name,
            "-density",
            str(density),
        ]
        with open(log_name, "w") as f:
            result = subprocess.run(
                msms_args, stdout=f, stderr=f, cwd=binary_path, timeout=300
            )
        if result.returncode != 0:
            print(
                f"*** An error occurred while executing the command: {' '.join(msms_args)}, see log file for details. *** "
            )
            raise RuntimeError(f"MSMS failed with return code {result.returncode}")
        verts, faces = parse_verts(vert_file=vert_file, face_file=face_file)
        success = True
    except Exception as e:
        raise e
    finally:
        # Only cleanup when successful; on failure keep artifacts for debugging
        if remove_files and success:
            silentremove(xyzr_name)
            silentremove(log_name)
            silentremove(vert_file)
            silentremove(face_file)
    return verts, faces


def pdb_to_surf_with_min(pdb_path, out_name=None, min_number=256, remove_files=True):
    """
    This function is useful to retrieve at least min_number vertices, which is useful for later use in DiffNets
    :param pdb_path:
    :param out_name:
    :param min_number:
    :return:
    """

    number_of_vertices = 0
    density = 1.0
    verts, faces = None, None
    while number_of_vertices < min_number:
        verts, faces = pdb_to_surf(
            pdb_path=pdb_path,
            out_name=out_name,
            density=density,
            remove_files=remove_files,
        )
        number_of_vertices = len(verts)
        density += 1
    return verts, faces


def pdb_to_alpha_complex(
    pdb_path,
    out_name=None,
    sbl_exe_path=None,
    remove_files=True,
    alpha_value=0.001,
    atom_pos=None,
    atom_radius=None,
    use_python_binding=True,
):
    """
    Generate alpha complex surface mesh.

    Args:
        use_python_binding: If True, use SBL Python bindings (faster).
                           If False, use subprocess to call SBL executable.
    """
    pdb_path = os.path.abspath(pdb_path)
    from alphasurf.utils.timing_stats import Timer

    if use_python_binding:
        # Python binding approach
        if atom_pos is None or atom_radius is None:
            with Timer("pdb_to_array"):
                parsed_data = parse_pdb_path(pdb_path, use_pqr=False)
            atom_pos = parsed_data[5]
            atom_radius = parsed_data[7]

        if atom_pos is None or atom_radius is None:
            raise RuntimeError(
                f"Could not parse atom positions or radii for {pdb_path}"
            )

        # Check for NaN or Inf values to prevent C++ segmentation fault
        if np.isnan(atom_pos).any() or np.isinf(atom_pos).any():
            raise RuntimeError(
                f"pdb_to_alpha_complex failed for : {pdb_path} data must be finite, check for nan or inf values in atom_pos"
            )
        if np.isnan(atom_radius).any() or np.isinf(atom_radius).any():
            raise RuntimeError(
                f"pdb_to_alpha_complex failed for : {pdb_path} data must be finite, check for nan or inf values in atom_radius"
            )

        with Timer("sbl_alpha_complex"):
            verts, faces, singular_edges, singular_faces = (
                cgal_alpha.compute_alpha_complex_from_atoms(
                    atom_pos.astype(np.float32),
                    atom_radius.astype(np.float32),
                    float(alpha_value),
                    1.4,
                    "singular+regular",
                )
            )

        return verts, faces, singular_edges, singular_faces

    else:
        # Subprocess approach (old method)
        pdb_dir = os.path.dirname(pdb_path)
        pdb_basename = os.path.basename(pdb_path).replace(".pdb", "")

        results_dir = os.path.join(pdb_dir, f"{pdb_basename}-results")
        if float(alpha_value) == int(alpha_value):
            alpha_dir_name = f"alpha-{int(alpha_value)}"
        else:
            alpha_dir_name = f"alpha-{alpha_value}"

        ply_file = os.path.join(results_dir, alpha_dir_name, f"{pdb_basename}.ply")
        log_file = os.path.join(pdb_dir, f"{pdb_basename}_alpha.log")

        if sbl_exe_path is None:
            sbl_exe_path = "/Users/gertnervictor/Documents/sbl/Core/Alpha_complex_of_molecular_model/src/Alpha_complex_of_molecular_model/build/sbl-alpha-complex-of-molecular-model.exe"

        try:
            cmd = [
                sbl_exe_path,
                "-f",
                pdb_path,
                "-a",
                str(alpha_value),
                "--ply",
                "--ply-filter",
                "singular+regular",
            ]

            with Timer("sbl_alpha_subprocess"):
                with open(log_file, "w") as f:
                    result = subprocess.run(
                        cmd, stdout=f, stderr=f, cwd=pdb_dir, timeout=1000
                    )

            if result.returncode != 0:
                raise RuntimeError(
                    f"SBL alpha complex failed with return code {result.returncode}"
                )

            if not os.path.exists(ply_file):
                raise RuntimeError(
                    f"SBL alpha complex did not generate expected .ply file: {ply_file}"
                )

            verts, faces = read_vertices_and_triangles(ply_file)

        finally:
            if remove_files:
                silentremove(log_file)
                if os.path.exists(results_dir):
                    shutil.rmtree(results_dir)

        return verts, faces, 0, 0


"""
Clean the MSMS surfae
"""


def remove_abnormal_triangles(verts, faces):
    """Remove abnormal triangles (angles ~180 or ~0) in the mesh

    Returns:
        pymesh.Mesh, a new mesh with abnormal faces removed
    """
    v1 = verts[faces[:, 0]]
    v2 = verts[faces[:, 1]]
    v3 = verts[faces[:, 2]]
    e1 = v3 - v2
    e2 = v1 - v3
    e3 = v2 - v1
    L1 = np.linalg.norm(e1, axis=1)
    L2 = np.linalg.norm(e2, axis=1)
    L3 = np.linalg.norm(e3, axis=1)
    cos1 = np.einsum("ij,ij->i", -e2, e3) / (L2 * L3)
    cos2 = np.einsum("ij,ij->i", e1, -e3) / (L1 * L3)
    cos3 = np.einsum("ij,ij->i", -e1, e2) / (L1 * L2)
    cos123 = np.concatenate(
        (cos1.reshape(-1, 1), cos2.reshape(-1, 1), cos3.reshape(-1, 1)), axis=-1
    )
    valid_faces = np.where(np.all(1 - cos123**2 > 1e-5, axis=-1))[0]
    faces_new = faces[valid_faces]
    return verts, faces_new


def check_mesh_validity(mesh, check_triangles=False):
    """Check if a mesh is valid by following criteria

    1) disconnected
    2) has isolated vertex
    3) face has duplicated vertices (same vertex on a face)
    4) has triangles with angle ~0 or ~180

    Returns
        four-tuple of bool: above criteria

    """
    mesh.enable_connectivity()
    verts, faces = mesh.vertices, mesh.faces

    # check if a manifold is all-connected using BFS
    visited = np.zeros(len(verts)).astype(bool)
    groups = []
    for ivert in range(len(verts)):
        if visited[ivert]:
            continue
        old_visited = visited.copy()
        queue = [ivert]
        visited[ivert] = True
        while queue:
            curr = queue.pop(0)
            for nbr in mesh.get_vertex_adjacent_vertices(curr):
                if not visited[nbr]:
                    queue.append(nbr)
                    visited[nbr] = True
        groups.append(np.where(np.logical_xor(old_visited, visited))[0])
    groups = sorted(groups, key=lambda x: len(x), reverse=True)
    assert sum(len(ig) for ig in groups) == sum(visited) == len(verts)
    disconnected = len(groups) > 1

    # check for isolated vertices
    valid_verts = np.unique(faces)
    has_isolated_verts = verts.shape[0] != len(valid_verts)

    # check for faces with duplicate vertices
    df = pd.DataFrame(faces)
    df = df[df.nunique(axis=1) == 3]
    has_duplicate_verts = df.shape[0] != mesh.num_faces

    # check for abnormal triangles
    if check_triangles:
        v1 = verts[faces[:, 0]]
        v2 = verts[faces[:, 1]]
        v3 = verts[faces[:, 2]]
        e1 = v3 - v2
        e2 = v1 - v3
        e3 = v2 - v1
        L1 = np.linalg.norm(e1, axis=1)
        L2 = np.linalg.norm(e2, axis=1)
        L3 = np.linalg.norm(e3, axis=1)
        cos1 = np.einsum("ij,ij->i", -e2, e3) / (L2 * L3)
        cos2 = np.einsum("ij,ij->i", e1, -e3) / (L1 * L3)
        cos3 = np.einsum("ij,ij->i", -e1, e2) / (L1 * L2)
        cos123 = np.concatenate(
            (cos1.reshape(-1, 1), cos2.reshape(-1, 1), cos3.reshape(-1, 1)), axis=-1
        )
        valid_faces = np.where(np.all(1 - cos123**2 >= 1e-5, axis=-1))[0]
        has_abnormal_triangles = faces.shape[0] != len(valid_faces)
    else:
        has_abnormal_triangles = False

    return disconnected, has_isolated_verts, has_duplicate_verts, has_abnormal_triangles


def mesh_simplification(
    verts,
    faces,
    out_ply,
    face_reduction_rate=1.0,
    min_vert_number=140,
    max_vert_number=50000,
    use_pymesh=True,
    surface_method="msms",
    obj_name=None,
):
    """
    Simplify and clean a mesh.
    Simplification is based on coarsening with iterative quadratic decimation
    Cleaning involves discarding small disconnected components, unreferenced faces and vertices...

    Most of the computation time resides in the coarsening.
    """

    # Extract base name for debug saves
    if out_ply is not None:
        base_name = os.path.splitext(os.path.basename(out_ply))[0]
    else:
        base_name = "debug_mesh"

    # remeshing to have a target number of vertices
    faces_num = max(min_vert_number * 2 + 1, int(face_reduction_rate * len(faces)))
    mesh = o3d.geometry.TriangleMesh(
        o3d.utility.Vector3dVector(verts), o3d.utility.Vector3iVector(faces)
    )
    # big_name = out_ply.replace('.ply', '_big.ply')
    # print('saving', big_name)
    # o3d.io.write_triangle_mesh(big_name, mesh)
    # save_debug_ply(mesh, base_name, "00_initial_mesh")
    # Clean with open3d
    if surface_method == "msms":
        mesh.remove_non_manifold_edges()
    # save_debug_ply(mesh, base_name, "01_after_remove_non_manifold_edges")
    mesh.remove_duplicated_vertices()
    # save_debug_ply(mesh, base_name, "02_after_remove_duplicated_vertices")
    mesh.remove_degenerate_triangles()
    # save_debug_ply(mesh, base_name, "03_after_remove_degenerate_triangles")
    mesh.remove_duplicated_triangles()
    # save_debug_ply(mesh, base_name, "04_after_remove_duplicated_triangles")

    mesh.remove_unreferenced_vertices()
    # save_debug_ply(mesh, base_name, "05_after_remove_unreferenced_vertices")
    if face_reduction_rate < 1.0:
        mesh = mesh.simplify_quadric_decimation(target_number_of_triangles=faces_num)
        # save_debug_ply(mesh, base_name, "06_after_msms_simplify_quadric_decimation")

    verts_out = np.asarray(mesh.vertices)
    faces_out = np.asarray(mesh.triangles)
    # save_debug_ply(mesh, base_name, "11_final_mesh_before_pymesh")

    # Default drop_ratio if not calculated (e.g. pymesh path or no clustering)
    drop_ratio = 0.0

    if not use_pymesh:
        # TODO time
        if surface_method == "alpha_complex":
            triangle_clusters, cluster_n_triangles, _ = (
                mesh.cluster_connected_triangles()
            )
            triangle_clusters = np.asarray(triangle_clusters)
            cluster_n_triangles = np.asarray(cluster_n_triangles)
        else:
            # Standard edge-based clustering for MSMS
            triangle_clusters, cluster_n_triangles, _ = (
                mesh.cluster_connected_triangles()
            )
            triangle_clusters = np.asarray(triangle_clusters)
            cluster_n_triangles = np.asarray(cluster_n_triangles)
        # save_debug_ply(mesh, base_name, "07_after_cluster_connected_triangles")

        if surface_method == "alpha_complex":
            # For alpha_complex, just keep the largest connected component
            largest_cluster_idx = np.argmax(cluster_n_triangles)
            triangles_to_remove = triangle_clusters != largest_cluster_idx
            dropped_faces = np.sum(triangles_to_remove)
            mesh.remove_triangles_by_mask(triangles_to_remove)
            # save_debug_ply(mesh, base_name, "08_alpha_complex_after_remove_triangles")
            mesh.remove_unreferenced_vertices()
            # save_debug_ply(mesh, base_name, "09_alpha_complex_after_remove_unreferenced_vertices")
            mesh.remove_degenerate_triangles()
            # save_debug_ply(mesh, base_name, "10_alpha_complex_after_remove_degenerate_triangles")
        else:
            # For MSMS, remove small clusters with cutoff check
            largest_cluster_size = np.max(cluster_n_triangles)
            cutoff = int(0.01 * largest_cluster_size)
            assert (
                (cluster_n_triangles >= cutoff).sum() == 1
            ), f"Cluster n triangles: {cluster_n_triangles}, cutoff: {cutoff}"

            triangles_to_remove = cluster_n_triangles[triangle_clusters] < cutoff
            dropped_faces = np.sum(triangles_to_remove)
            mesh.remove_triangles_by_mask(triangles_to_remove)
            # save_debug_ply(mesh, base_name, "08_msms_after_remove_triangles")
            mesh.remove_unreferenced_vertices()
            # save_debug_ply(mesh, base_name, "09_msms_after_remove_unreferenced_vertices")
            mesh.remove_degenerate_triangles()
            # save_debug_ply(mesh, base_name, "10_msms_after_remove_degenerate_triangles")

        # Log warning if too many faces were removed
        total_faces = len(triangle_clusters)
        if total_faces > 0:
            drop_ratio = dropped_faces / total_faces
        else:
            drop_ratio = 0.0

        if drop_ratio > 0.1:
            msg = f"WARNING: Dropped {drop_ratio:.1%} of faces in component filtering"
            if obj_name:
                msg += f" for {obj_name}"
            print(msg)
        verts_out = np.asarray(mesh.vertices).astype(np.float32)
        faces_out = np.asarray(mesh.triangles).astype(np.int32)

    if use_pymesh:
        import pymesh

        # cleaning the mesh with Pymesh
        mesh_py = pymesh.form_mesh(verts_out, faces_out)
        disconnected, isolated_verts, duplicate_verts, abnormal_triangles = (
            check_mesh_validity(mesh_py, check_triangles=True)
        )
        _ = not (
            disconnected or isolated_verts or duplicate_verts or abnormal_triangles
        )

        # mesh_py = pymesh.form_mesh_py(verts, faces)
        mesh_py, _ = pymesh.remove_duplicated_vertices(mesh_py, 1e-6)  # duplicate
        mesh_py, _ = pymesh.remove_degenerated_triangles(
            mesh_py, 100
        )  # colinear vertices

        num_verts = mesh_py.num_vertices
        iteration = 0
        while iteration < 10:
            mesh_py, _ = pymesh.collapse_short_edges(mesh_py, rel_threshold=0.1)
            mesh_py, _ = pymesh.remove_obtuse_triangles(mesh_py, 170.0, 100)
            if abs(mesh_py.num_vertices - num_verts) < 20:
                break
            num_verts = mesh_py.num_vertices
            iteration += 1

        mesh_py = pymesh.resolve_self_intersection(mesh_py)
        mesh_py, _ = pymesh.remove_duplicated_faces(mesh_py)  # easy
        mesh_py, _ = pymesh.remove_obtuse_triangles(mesh_py, 179.0, 100)
        mesh_py = pymesh.form_mesh(
            *remove_abnormal_triangles(mesh_py.vertices, mesh_py.faces)
        )
        mesh_py, _ = pymesh.remove_isolated_vertices(
            mesh_py
        )  # vertices not in faces, easy with index
        verts_py, faces_py = (
            np.array(mesh_py.vertices, dtype=np.float32),
            np.array(mesh_py.faces, dtype=np.int32),
        )

        disconnected, isolated_verts, duplicate_verts, abnormal_triangles = (
            check_mesh_validity(mesh_py, check_triangles=True)
        )
        is_valid_mesh = not (
            disconnected or isolated_verts or duplicate_verts or abnormal_triangles
        )

        # Just a few assessments of the usefullness of PyMesh
        # size_diff = len(verts) - len(faces_py)
        # print('original', len(verts), 'coarsened', len(verts_out), 'corrected', len(verts_py), 'size_diff', size_diff,
        #       'valid_first', is_valid_mesh_first, 'valid', is_valid_mesh)
        # a = faces_py - faces_out
        # a = faces_py - faces
        # b = verts_py - verts
        # print(np.max(np.abs(a)), np.max(np.abs(b)))
        # if not is_valid_mesh_first:
        #     print(is_valid_mesh_first, is_valid_mesh)
        if not is_valid_mesh:
            raise ValueError("Mesh is not valid")
        verts_out = verts_py
        faces_out = faces_py

    if verts_out.shape[0] > max_vert_number:
        raise ValueError(f"Too many vertices in the mesh: {verts_out.shape[0]}")
    if verts_out.shape[0] < min_vert_number:
        raise ValueError(f"Not enough vertices in the mesh: {verts_out.shape[0]}")

    # save to ply
    if out_ply is not None:
        mesh = o3d.geometry.TriangleMesh(
            o3d.utility.Vector3dVector(verts_out), o3d.utility.Vector3iVector(faces_out)
        )
        o3d.io.write_triangle_mesh(out_ply, mesh, write_vertex_normals=True)

    verts_out = verts_out.astype(np.float32)
    faces_out = faces_out.astype(np.int32)
    # remove duplicate
    mesh = trimesh.Trimesh(vertices=verts_out, faces=faces_out, preprocess=True)
    # save_debug_ply(mesh, base_name, "12_after_trimesh_preprocess")
    verts_out = np.array(mesh.vertices).astype(np.float32)
    faces_out = np.array(mesh.faces).astype(np.int32)

    # save_debug_ply(mesh, base_name, "13_final_mesh_after_trimesh_preprocess")

    return verts_out, faces_out, drop_ratio


def get_surface(
    pdb_path="../../data/example_files/4kt3.pdb",
    out_ply_path=None,
    face_reduction_rate=1.0,
    min_vert_number=140,
    max_vert_number=50000,
    use_pymesh=True,
    surface_method="msms",
    sbl_exe_path=None,
    alpha_value=0.1,
):
    # # Check that msms and with_min gives the right output
    if surface_method == "msms":
        verts, faces = pdb_to_surf_with_min(pdb_path, min_number=min_vert_number)
    elif surface_method == "alpha_complex":
        verts, faces, _, _ = pdb_to_alpha_complex(
            pdb_path, sbl_exe_path=sbl_exe_path, alpha_value=alpha_value
        )
    else:
        raise ValueError(f"Unknown surface method: {surface_method}")

    verts, faces, drop_ratio = mesh_simplification(
        verts=verts,
        faces=faces,
        out_ply=out_ply_path,
        face_reduction_rate=face_reduction_rate,
        min_vert_number=min_vert_number,
        max_vert_number=max_vert_number,
        use_pymesh=use_pymesh,
        surface_method=surface_method,
    )
    return verts, faces


def read_vertices_and_triangles(ply_file):
    """
    Just a small wrapper to retrieve directly the vertices and faces as np arrays with the right dtypes
    :param ply_file:
    :return:
    """
    mesh = o3d.io.read_triangle_mesh(filename=ply_file)
    vertices = np.asarray(mesh.vertices, dtype=np.float32)
    faces = np.asarray(mesh.triangles, dtype=np.int32)
    return vertices, faces


def add_normal_noise(verts, faces, sigma=0.3, normals=None, clip_sigma=3.0, seed=None):
    """
    Displace vertices along their normals with Gaussian noise.

    This is the recommended noise type for protein surfaces as it:
    - Preserves surface topology better than isotropic noise
    - Mimics physical uncertainty in atomic radii
    - Maintains local shape features (valleys stay valleys, ridges stay ridges)

    Args:
        verts: (N, 3) array of vertex positions
        faces: (M, 3) array of face indices
        sigma: Standard deviation of displacement in Angstroms (default: 0.3)
               Recommended values: 0.1-0.3 (conservative), 0.3-0.5 (moderate), 0.5-1.0 (aggressive)
        normals: (N, 3) array of pre-computed vertex normals (auto-computed if None)
        clip_sigma: Clip noise to ±clip_sigma*sigma to prevent extreme outliers (None to disable)
        seed: Random seed for reproducibility (useful for creating fixed augmented views)

    Returns:
        verts_noisy: (N, 3) array of displaced vertex positions
    """
    import igl

    if seed is not None:
        np.random.seed(seed)

    if normals is None:
        normals = igl.per_vertex_normals(verts, faces)

    noise = np.random.randn(len(verts), 1) * sigma

    if clip_sigma is not None:
        noise = np.clip(noise, -clip_sigma * sigma, clip_sigma * sigma)

    verts_noisy = verts + normals * noise

    return verts_noisy.astype(np.float32)


def add_surface_noise(
    verts,
    faces,
    sigma=0.3,
    normals=None,
    clip_sigma=3.0,
    seed=None,
):
    """
    Add noise to surface vertices along their normals.

    Args:
        verts: (N, 3) array of vertex positions
        faces: (M, 3) array of face indices
        sigma: Standard deviation of displacement in Angstroms
        normals: (N, 3) array of pre-computed vertex normals (auto-computed if None)
        clip_sigma: Clip noise to ±clip_sigma*sigma to prevent extreme outliers
        seed: Random seed for reproducibility

    Returns:
        verts_noisy: (N, 3) array of displaced vertex positions
    """
    return add_normal_noise(
        verts, faces, sigma=sigma, normals=normals, clip_sigma=clip_sigma, seed=seed
    )


if __name__ == "__main__":
    pdb = "../../data/example_files/4kt3.pdb"
    outname = "../../data/example_files/test"
    vert_file = outname + ".vert"
    face_file = outname + ".face"

    # # Check that msms and with_min gives the right output
    # verts, faces = pdb_to_surf(pdb, out_name=outname, density=1., remove_files=False)
    # verts, faces = pdb_to_surf_with_min(pdb, out_name=outname, min_number=256, remove_files=False)
    # mesh = o3d.geometry.TriangleMesh(o3d.utility.Vector3dVector(verts), o3d.utility.Vector3iVector(faces))
    # # compute normal for rendering
    # mesh.compute_triangle_normals()
    # mesh.compute_vertex_normals()
    # o3d.visualization.draw_geometries([mesh])

    # Now simplify this into a coarser mesh (upper bound), and turn it into a corrected ply file
    # ply_file = "../../data/example_files/example_mesh.ply"
    # verts, faces = parse_verts(vert_file, face_file)
    # mesh_simplification(verts=verts,
    #                     faces=faces,
    #                     out_ply=ply_file,
    #                     use_pymesh=False,
    #                     face_reduction_rate=0.2)
    # mesh_reduced = o3d.io.read_triangle_mesh(ply_file)
    # mesh_reduced.compute_triangle_normals()
    # o3d.visualization.draw_geometries([mesh_reduced])

    # MSMS surface generation
    verts, faces = get_surface(
        pdb_path="../../data/example_files/4kt3.pdb",
        out_ply_path="../../data/example_files/example_mesh_msms.ply",
        min_vert_number=140,
        use_pymesh=False,
        face_reduction_rate=0.1,
        surface_method="msms",
    )

    # # Alpha complex surface generation
    # verts, faces = get_surface(pdb_path="../../data/example_files/4kt3.pdb",
    #                            out_ply_path="../../data/example_files/example_mesh_alpha.ply",
    #                            min_vert_number=140,
    #                            use_pymesh=False,
    #                            face_reduction_rate=0.1,
    #                            surface_method='alpha_complex')
